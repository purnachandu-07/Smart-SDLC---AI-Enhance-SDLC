# conversation_handler.py
import json
import logging
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import re

class ConversationHandler:
    """
    Handles conversation flow, context management, and SDLC-specific assistance
    """
    
    def __init__(self, model_pipeline):
        self.model_pipeline = model_pipeline
        self.conversation_history = []
        self.context = {
            "current_phase": None,
            "project_context": {},
            "user_preferences": {},
            "active_tasks": []
        }
        self.sdlc_phases = [
            "requirements", "design", "development", 
            "testing", "deployment", "maintenance"
        ]
        
        # SDLC-specific prompts and responses
        self.sdlc_prompts = {
            "requirements": {
                "keywords": ["requirement", "user story", "functional", "non-functional", "acceptance criteria"],
                "suggestions": [
                    "Would you like me to help analyze requirements from a document?",
                    "I can help generate user stories from your requirements.",
                    "Need assistance with acceptance criteria definition?"
                ]
            },
            "design": {
                "keywords": ["architecture", "design pattern", "UML", "database", "API"],
                "suggestions": [
                    "I can help with system architecture recommendations.",
                    "Would you like suggestions for design patterns?",
                    "Need help with database schema design?"
                ]
            },
            "development": {
                "keywords": ["code", "programming", "function", "class", "algorithm"],
                "suggestions": [
                    "I can generate code based on your requirements.",
                    "Need help with specific programming problems?",
                    "Would you like me to review your code structure?"
                ]
            },
            "testing": {
                "keywords": ["test", "unit test", "integration", "bug", "quality"],
                "suggestions": [
                    "I can generate test cases for your code.",
                    "Need help with test automation strategies?",
                    "Would you like assistance with bug analysis?"
                ]
            },
            "deployment": {
                "keywords": ["deploy", "CI/CD", "docker", "kubernetes", "production"],
                "suggestions": [
                    "I can help with deployment strategies.",
                    "Need assistance with CI/CD pipeline setup?",
                    "Would you like help with containerization?"
                ]
            },
            "maintenance": {
                "keywords": ["maintenance", "refactor", "optimization", "documentation"],
                "suggestions": [
                    "I can help with code refactoring suggestions.",
                    "Need assistance with performance optimization?",
                    "Would you like help generating documentation?"
                ]
            }
        }
    
    def detect_sdlc_phase(self, message: str) -> Optional[str]:
        """
        Detect which SDLC phase the user is asking about
        """
        message_lower = message.lower()
        phase_scores = {}
        
        for phase, data in self.sdlc_prompts.items():
            score = 0
            for keyword in data["keywords"]:
                if keyword in message_lower:
                    score += 1
            phase_scores[phase] = score
        
        # Return phase with highest score, or None if no matches
        max_score = max(phase_scores.values())
        if max_score > 0:
            return max(phase_scores, key=phase_scores.get)
        return None
    
    def generate_contextual_response(self, user_message: str) -> str:
        """
        Generate AI response with SDLC context awareness
        """
        # Detect SDLC phase
        detected_phase = self.detect_sdlc_phase(user_message)
        if detected_phase:
            self.context["current_phase"] = detected_phase
        
        # Build context-aware prompt
        system_prompt = self.build_system_prompt(detected_phase)
        full_prompt = f"{system_prompt}\n\nUser: {user_message}\nAssistant:"
        
        # Generate response using the model
        try:
            response = self.model_pipeline(
                full_prompt,
                max_length=512,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.model_pipeline.tokenizer.eos_token_id
            )[0]['generated_text']
            
            # Extract only the assistant's response
            response = response.split("Assistant:")[-1].strip()
            
            # Add contextual suggestions if appropriate
            if detected_phase and len(response) < 100:
                suggestions = self.sdlc_prompts[detected_phase]["suggestions"]
                response += f"\n\nðŸ’¡ {suggestions[0]}"
            
            return response
            
        except Exception as e:
            logging.error(f"Error generating response: {e}")
            return "I apologize, but I encountered an error. Please try rephrasing your question."
    
    def build_system_prompt(self, phase: Optional[str] = None) -> str:
        """
        Build system prompt based on current context and SDLC phase
        """
        base_prompt = """You are an AI assistant specialized in Software Development Lifecycle (SDLC). 
You help developers with requirements analysis, design, coding, testing, deployment, and maintenance tasks.
Provide practical, actionable advice and be concise but helpful."""
        
        if phase:
            phase_context = f"\nCurrently focusing on: {phase.upper()} phase of SDLC."
            base_prompt += phase_context
        
        if self.context["project_context"]:
            project_info = f"\nProject context: {self.context['project_context']}"
            base_prompt += project_info
        
        return base_prompt
    
    def add_to_history(self, user_message: str, ai_response: str):
        """
        Add conversation turn to history
        """
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "user_message": user_message,
            "ai_response": ai_response,
            "context": self.context.copy()
        })
        
        # Keep only last 10 conversations to manage memory
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
    
    def get_quick_suggestions(self, phase: str = None) -> List[str]:
        """
        Get quick suggestions based on current or specified phase
        """
        target_phase = phase or self.context.get("current_phase", "development")
        if target_phase in self.sdlc_prompts:
            return self.sdlc_prompts[target_phase]["suggestions"]
        return [
            "How can I help with your development task?",
            "Would you like code generation assistance?",
            "Need help with testing or documentation?"
        ]


# chat_routes.py
import gradio as gr
from typing import List, Tuple
import json

class ChatInterface:
    """
    Gradio interface for the SDLC chatbot
    """
    
    def __init__(self, conversation_handler: ConversationHandler):
        self.conversation_handler = conversation_handler
        self.chat_history = []
    
    def chat_response(self, message: str, history: List[Tuple[str, str]]) -> Tuple[str, List[Tuple[str, str]]]:
        """
        Process chat message and return response with updated history
        """
        if not message.strip():
            return "", history
        
        # Generate AI response
        ai_response = self.conversation_handler.generate_contextual_response(message)
        
        # Add to conversation handler history
        self.conversation_handler.add_to_history(message, ai_response)
        
        # Update chat history for Gradio
        history.append((message, ai_response))
        
        return "", history
    
    def clear_chat(self):
        """
        Clear chat history
        """
        self.conversation_handler.conversation_history = []
        return [], ""
    
    def get_phase_suggestions(self, phase: str) -> str:
        """
        Get suggestions for a specific SDLC phase
        """
        suggestions = self.conversation_handler.get_quick_suggestions(phase)
        return "\n".join([f"â€¢ {suggestion}" for suggestion in suggestions])
    
    def create_interface(self) -> gr.Interface:
        """
        Create the Gradio interface for the chatbot
        """
        with gr.Blocks(
            title="Smart SDLC Assistant",
            theme=gr.themes.Soft(),
            css="""
            .chatbot-container {
                height: 500px;
                overflow-y: auto;
            }
            .suggestion-box {
                background-color: #f0f8ff;
                padding: 10px;
                border-radius: 5px;
                margin: 10px 0;
            }
            .phase-buttons {
                display: flex;
                gap: 10px;
                flex-wrap: wrap;
                margin: 10px 0;
            }
            """
        ) as interface:
            
            gr.Markdown(
                """
                # ðŸ¤– Smart SDLC Assistant
                
                Your AI-powered companion for Software Development Lifecycle tasks.
                Ask me anything about requirements, design, development, testing, deployment, or maintenance!
                """,
                elem_classes=["header"]
            )
            
            with gr.Row():
                # Main chat area
                with gr.Column(scale=3):
                    chatbot = gr.Chatbot(
                        label="SDLC Assistant Chat",
                        height=400,
                        elem_classes=["chatbot-container"],
                        avatar_images=("ðŸ‘¤", "ðŸ¤–")
                    )
                    
                    with gr.Row():
                        msg_input = gr.Textbox(
                            placeholder="Ask me about any SDLC task...",
                            label="Your Message",
                            scale=4
                        )
                        send_btn = gr.Button("Send", variant="primary", scale=1)
                    
                    with gr.Row():
                        clear_btn = gr.Button("Clear Chat", variant="secondary")
                        export_btn = gr.Button("Export History", variant="secondary")
                
                # Sidebar with quick actions
                with gr.Column(scale=1):
                    gr.Markdown("### ðŸš€ Quick Actions")
                    
                    # SDLC Phase buttons
                    gr.Markdown("**SDLC Phases:**")
                    with gr.Column():
                        req_btn = gr.Button("ðŸ“‹ Requirements", size="sm")
                        design_btn = gr.Button("ðŸŽ¨ Design", size="sm")
                        dev_btn = gr.Button("ðŸ’» Development", size="sm")
                        test_btn = gr.Button("ðŸ§ª Testing", size="sm")
                        deploy_btn = gr.Button("ðŸš€ Deployment", size="sm")
                        maintain_btn = gr.Button("ðŸ”§ Maintenance", size="sm")
                    
                    # Suggestions area
                    suggestions_area = gr.Textbox(
                        label="ðŸ’¡ Suggestions",
                        value="Welcome! Select a phase above or ask me anything about SDLC.",
                        lines=8,
                        interactive=False,
                        elem_classes=["suggestion-box"]
                    )
            
            # Example questions
            gr.Markdown(
                """
                ### ðŸŽ¯ Example Questions:
                - "Help me write user stories for a shopping cart feature"
                - "Generate Python code for user authentication"
                - "Create unit tests for my calculator function"
                - "What are the best practices for API design?"
                - "How do I set up a CI/CD pipeline?"
                """
            )
            
            # Event handlers
            def handle_send(message, history):
                return self.chat_response(message, history)
            
            def handle_clear():
                return self.clear_chat()
            
            def handle_phase_click(phase):
                return self.get_phase_suggestions(phase)
            
            def export_history():
                if self.conversation_handler.conversation_history:
                    return json.dumps(self.conversation_handler.conversation_history, indent=2)
                return "No conversation history to export."
            
            # Wire up events
            send_btn.click(
                handle_send,
                inputs=[msg_input, chatbot],
                outputs=[msg_input, chatbot]
            )
            
            msg_input.submit(
                handle_send,
                inputs=[msg_input, chatbot],
                outputs=[msg_input, chatbot]
            )
            
            clear_btn.click(
                handle_clear,
                outputs=[chatbot, msg_input]
            )
            
            # Phase button clicks
            req_btn.click(lambda: handle_phase_click("requirements"), outputs=suggestions_area)
            design_btn.click(lambda: handle_phase_click("design"), outputs=suggestions_area)
            dev_btn.click(lambda: handle_phase_click("development"), outputs=suggestions_area)
            test_btn.click(lambda: handle_phase_click("testing"), outputs=suggestions_area)
            deploy_btn.click(lambda: handle_phase_click("deployment"), outputs=suggestions_area)
            maintain_btn.click(lambda: handle_phase_click("maintenance"), outputs=suggestions_area)
            
            export_btn.click(
                export_history,
                outputs=gr.Textbox(label="Exported History", lines=10)
            )
        
        return interface


# main_chatbot.py - Integration with the main application
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch

class SmartSDLCChatbot:
    """
    Main chatbot class that integrates with the Smart SDLC application
    """
    
    def __init__(self):
        self.model_name = "ibm-granite/granite-3.3-2b-instruct"
        self.model_pipeline = None
        self.conversation_handler = None
        self.chat_interface = None
        self.setup_model()
    
    def setup_model(self):
        """
        Initialize the IBM Granite model
        """
        try:
            print("Loading IBM Granite model...")
            
            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            # Create pipeline
            self.model_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            
            # Initialize conversation handler
            self.conversation_handler = ConversationHandler(self.model_pipeline)
            
            # Initialize chat interface
            self.chat_interface = ChatInterface(self.conversation_handler)
            
            print("âœ… Chatbot initialized successfully!")
            
        except Exception as e:
            print(f"âŒ Error initializing chatbot: {e}")
            # Fallback to a simple response system
            self.setup_fallback()
    
    def setup_fallback(self):
        """
        Setup fallback system if model loading fails
        """
        print("Setting up fallback response system...")
        
        class FallbackPipeline:
            def __call__(self, prompt, **kwargs):
                # Simple rule-based responses
                responses = {
                    "requirements": "I can help you with requirements analysis. Try uploading a document or describing your user stories.",
                    "design": "For design assistance, I can help with architecture patterns, database design, and API specifications.",
                    "development": "I can help generate code, review existing code, and suggest improvements.",
                    "testing": "I can help create test cases, suggest testing strategies, and identify potential bugs.",
                    "deployment": "I can assist with deployment strategies, CI/CD setup, and production considerations.",
                    "maintenance": "I can help with code refactoring, performance optimization, and documentation."
                }
                
                prompt_lower = prompt.lower()
                for key, response in responses.items():
                    if key in prompt_lower:
                        return [{"generated_text": f"{prompt}\nAssistant: {response}"}]
                
                return [{"generated_text": f"{prompt}\nAssistant: I'm here to help with your SDLC tasks. What specific area would you like assistance with?"}]
        
        self.model_pipeline = FallbackPipeline()
        self.conversation_handler = ConversationHandler(self.model_pipeline)
        self.chat_interface = ChatInterface(self.conversation_handler)
    
    def launch_interface(self, share=True, debug=True):
        """
        Launch the Gradio interface
        """
        if self.chat_interface:
            interface = self.chat_interface.create_interface()
            return interface.launch(share=share, debug=debug)
        else:
            print("âŒ Chat interface not initialized")
            return None
    
    def get_response(self, message: str) -> str:
        """
        Get a response from the chatbot (for programmatic use)
        """
        if self.conversation_handler:
            return self.conversation_handler.generate_contextual_response(message)
        return "Chatbot not initialized"


# Example usage for Google Colab
if __name__ == "__main__":
    # Initialize the chatbot
    chatbot = SmartSDLCChatbot()
    
    # Launch the interface
    print("ðŸš€ Launching Smart SDLC Chatbot...")
    interface = chatbot.launch_interface(share=True, debug=True)
    
    # Example of programmatic usage
    print("\n" + "="*50)
    print("Example interactions:")
    print("="*50)
    
    example_messages = [
        "Help me write user stories for a login system",
        "Generate Python code for a REST API endpoint",
        "Create unit tests for a calculator function",
        "What are the best practices for database design?"
    ]
    
    for message in example_messages:
        response = chatbot.get_response(message)
        print(f"\nUser: {message}")
        print(f"Assistant: {response[:200]}...")